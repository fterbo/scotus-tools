#!/usr/bin/env python

# Copyright (c) 2018  Floyd Terbo

import argparse
import datetime
import json
import logging
import os
import os.path
import sys
import time
import urllib

import BeautifulSoup as BS
import requests

import scotus.util

HEADERS = {"User-Agent" : "SCOTUS Docket Grabber (https://github.com/fterbo/scotus-tools)"}
BASE = "https://www.supremecourt.gov/rss/cases/JSON/"

def GET (url):
  logging.debug("GET: %s" % (url))
  return requests.get(url, headers=HEADERS)

class SkipDocket(Exception): pass

def parse_args ():
  parser = argparse.ArgumentParser()
  parser.add_argument("-t", "--term", dest="term", type=int)
  parser.add_argument("-o", "--orig", dest="orig", default = False, action="store_true")
  parser.add_argument("-n", "--docket-num", dest="docket_num", type=int)
  parser.add_argument("-a", "--action", dest="action", type=str, default="scan-petition")
  parser.add_argument("--dry-run", dest="dry_run", action="store_true")
  parser.add_argument("--debug", dest="debug", action="store_true")
  parser.add_argument("--stop", dest="stop", type=int, default=0)
  parser.add_argument("--root", dest="root", type=str, default=".")
  args = parser.parse_args()
  return args

def buildDocketStr (opts, num = None):
  if not num:
    num = opts.docket_num
  if opts.orig:
    return "22O%d" % (num)
  elif opts.action.count("application"):
    return "%dA%d" % (opts.term, num)

  return "%d-%d" % (opts.term, num)

def updateDockets (path, opts):
  nextnum = opts.docket_num
  while True:
    try:
      if nextnum > opts.stop:
        return

      newpath = "%s/%d/" % (path, nextnum)
      docketstr = buildDocketStr(opts, nextnum)

      if not os.path.exists(newpath):
        if nextnum < opts.stop:
          raise SkipDocket()
        return

      dinfopath = "%s/docket.json" % (newpath)
      if not os.path.exists(dinfopath):
        raise SkipDocket()

      docket_obj = json.loads(open(dinfopath, "rb").read())
      docket = scotus.util.DocketStatusInfo(docket_obj)

      # If the docket has been dismissed, denied, or has an issued judgment, you'll have to update it
      # by hand if you know it has new data
      if not docket.pending:
        raise SkipDocket()

      # Don't update the docket if it's scheduled for a conference in the future
      if docket.distributed:
        if datetime.date.today() < docket.distributed[-1][1]:
          raise SkipDocket()

      logging.info("Updating pending docket (%s)" % (docketstr))
      if opts.dry_run:
        raise SkipDocket()

      url = "%s%s.json" % (BASE, docketstr)
      r = GET(url)
      if r.status_code != 200:
        logging.info("Skipping <%s> with bad status [%d]" % (url, r.status_code))
        raise SkipDocket()

      with open(dinfopath, "w+") as f:
        f.write(json.dumps(r.json()))

      nextnum += 1

    except SkipDocket:
      nextnum += 1

    


# TODO: We really should use some exceptions to better handle control flow
def scanPetitions (path, opts):
  nextnum = opts.docket_num
  while True:
    newpath = "%s/%d/" % (path, nextnum)
    docketstr = buildDocketStr(opts, nextnum)

    if os.path.exists("%s/docket.json" % (newpath)):
      # We already scanned this docket
      nextnum += 1
      continue

    url = "%s%s.json" % (BASE, docketstr)
    r = GET(url)
    if r.status_code != 200:
      if opts.stop and nextnum < opts.stop:
        logging.info("Skipping <%s> with no docket [STATUS: %d]" % (url, r.status_code))
        nextnum += 1
        continue
      else:
        logging.info("Stopped for URL <%s> with code %d" % (url, r.status_code))
        break

    docket_obj = r.json()
    founditem = None
    for item in docket_obj["ProceedingsandOrder"]:
      try:
        for link in item["Links"]:
          if (link["Description"] == "Petition" or
              link["Description"] == "Motion for Leave to File a Bill of Complaint"):
            founditem = item
            break
        if founditem:
          break
      except KeyError:
        # Likely original extension of time to file
        logging.debug("[%s] No links: %s" % (docketstr, item["Text"]))
        continue

    try:
      os.makedirs(newpath)
    except OSError:
      pass # Should check errno, but probably already exists

    with open("%s/docket.json" % (newpath), "w+") as f:
      f.write(json.dumps(r.json()))

    if not founditem:
      logging.error("Couldn't find a petition for docket %s" % (docketstr))
      nextnum += 1
      continue

    match = list(set(founditem["Text"].split()) & scotus.util.PETITION_TYPES)
    try:
      casename = scotus.util.buildCasename(docket_obj)
    except scotus.util.SCOTUSError as e:
      logging.exception(str(e))

    if len(match) == 0:
      logging.warning("[%s] Unknown petition type for: %s" % (docketstr, founditem["Text"]))
    elif len(match) == 1:
      logging.info("[%s] (%s) %s" % (docketstr, match[0], casename))
    elif len(match) > 1:
      logging.info("[%s] Too many type matches for: %s" % (docketstr, founditem["Text"]))

    for link in founditem["Links"]:
      if link["Description"] in scotus.util.PETITION_LINKS:
        outpath = "%s/%s" % (newpath, link["File"])
        if os.path.exists(outpath):
          continue
        logging.debug("[%s] Downloading %s" % (docketstr, link["File"]))
        dl = GET(link["DocumentUrl"])
        if dl.status_code != 200:
          logging.error("[%s] FAILED: %d" % (docketstr, dl.status_code))
          continue
        with open(outpath, "w+") as f:
          f.write(dl.content)

    nextnum += 1
        

def scanApplications (path, opts):
  nextnum = opts.docket_num
  while True:
    newpath = "%s/A/%d"% (path, nextnum)
    docketstr = buildDocketStr(opts, nextnum)

    if os.path.exists("%s/docket.json" % (newpath)):
      # We already scanned this docket
      nextnum += 1
      continue

    url = "%s%s.json" % (BASE, docketstr)
    r = GET(url)
    if r.status_code != 200:
      if opts.stop and nextnum < opts.stop:
        logging.info("Skipping <%s> with no docket [STATUS: %d]" % (url, r.status_code))
        nextnum += 1
        continue
      else:
        logging.info("Stopped for URL <%s> with code %d" % (url, r.status_code))
        break

    try:
      os.makedirs(newpath)
    except OSError:
      pass # Should check errno, but probably already exists

    docket_obj = r.json()
    with open("%s/docket.json" % (newpath), "w+") as f:
      f.write(json.dumps(r.json()))

    founditem = None
    for item in docket_obj["ProceedingsandOrder"]:
      try:
        for link in item["Links"]:
          if (link["Description"] == "Main Document"):
            logging.info("[%s] %s" % (docketstr, item["Text"]))
            founditem = item
            break
        if founditem:
          break
      except KeyError:
        # Likely original extension of time to file
        logging.debug("[%s] No links: %s" % (docketstr, item["Text"]))
        continue

    if not founditem:
      logging.error("Couldn't find application text for docket %s" % (docketstr))
      nextnum += 1
      continue



# TODO: we should optionally allow checking last-modified with HEAD before we grab if the caller
# is doing a larger scan where many are unlikely to be updated since last scan
def downloadFull (path, opts):
  docketstr = buildDocketStr(opts)
  newpath = "%s/%d/" % (path, opts.docket_num)
  try:
    os.makedirs(newpath)
  except OSError:
    pass

  url = "%s%s.json" % (BASE, docketstr)
  r = GET(url)
  if r.status_code != 200:
    logging.error("Could not get docket metadata <%s> with code %d" % (url, r.status_code))
    sys.exit(1)

  docket_obj = r.json()
  with open("%s/docket.json" % (newpath), "w+") as df:
    df.write(json.dumps(docket_obj))

  for item in docket_obj["ProceedingsandOrder"]:
    if "Links" not in item:
      continue
    for link in item["Links"]:
      if "DocumentUrl" not in link:
        logging.warning("Found link without DocumentUrl: %s" % (link["Description"]))
        continue
      fname = urllib.unquote_plus(link["DocumentUrl"].split("/")[-1])
      outpath = "%s/%s" % (newpath, fname)
      if os.path.exists(outpath):
        continue
      logging.info("[%s] Downloading %s" % (docketstr, fname))
      dl = GET(link["DocumentUrl"])
      if dl.status_code != 200:
        logging.error("[%s] FAILED: %d" % (docketstr, dl.status_code))
        continue
      with open(outpath, "w+") as f:
        f.write(dl.content)


ACTIONS = {
  "scan-petition" : scanPetitions,
  "petitions" : scanPetitions,
  "scan-applications" : scanApplications,
  "download-full" : downloadFull,
  "download" : downloadFull,
  "update" : updateDockets,
}

if __name__ == '__main__':
  opts = parse_args()

  if opts.debug:
    logging.basicConfig(level=logging.DEBUG)
  else:
    logging.basicConfig(level=logging.INFO)

  if opts.orig:
    path = "%s/Orig/dockets" % (opts.root)
  else:
    path = "%s/OT-%d/dockets" % (opts.root, opts.term)

  try:
    os.makedirs(path)
  except OSError:
    pass

  func = ACTIONS[opts.action]
  func(path, opts)
